{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MNIST image list\n",
    "#set of handwritten characters from 0-9\n",
    "#first download and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "# 0 0 1 0 0 0 0 0 \n",
    "\n",
    "def savetxt(filename, ndarray):\n",
    "    with open(filename, 'w') as f:\n",
    "        labels = list(map(' '.join, np.eye(10, dtype=np.uint).astype(str)))\n",
    "        for row in ndarray:\n",
    "            row_str = row.astype(str)\n",
    "            label_str = labels[row[-1]]\n",
    "            feature_str = ' '.join(row_str[:-1])\n",
    "            f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "            \n",
    "def loadData(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x3080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))[0]\n",
    "            if n != cimg:\n",
    "            \n",
    "            \n",
    "                raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n",
    "            crow = struct.unpack('>I', gz.read(4))[0]\n",
    "            ccol = struct.unpack('>I', gz.read(4))[0]\n",
    "            if crow != 28 or ccol != 28:\n",
    "                raise Exception('Invalid file: expected 28 rows/cols per image.')\n",
    "            # Read data.\n",
    "            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, crow * ccol))\n",
    "\n",
    "def loadLabels(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x1080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))\n",
    "            if n[0] != cimg:\n",
    "                raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n",
    "            # Read labels.\n",
    "            res = np.fromstring(gz.read(cimg), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(dataSrc, labelsSrc, cimg):\n",
    "    data = loadData(dataSrc, cimg)\n",
    "    labels = loadLabels(labelsSrc, cimg)\n",
    "    return np.hstack((data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Writing train text file...\n",
      "Done.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Writing test text file...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#Import dataset\n",
    "train_path = './Train-28x28_cntk_text.txt'\n",
    "test_path = './Test-28x28_cntk_text.txt'\n",
    "\n",
    "train = load('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', 60000)\n",
    "\n",
    "print ('Writing train text file...')\n",
    "\n",
    "savetxt(train_path, train)\n",
    "\n",
    "print ('Done.')\n",
    "\n",
    "test = load('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "    'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "print ('Writing test text file...')\n",
    "\n",
    "savetxt(test_path, test)\n",
    "\n",
    "print ('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.train import Trainer, minibatch_size_schedule \n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT\n",
    "from cntk.device import cpu, try_set_default_device\n",
    "from cntk.learners import adadelta, learning_rate_schedule, UnitType\n",
    "from cntk.ops import relu, element_times, constant\n",
    "from cntk.layers import Dense, Sequential, For\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.metrics import classification_error\n",
    "from cntk.train.training_session import *\n",
    "from cntk.logging import ProgressPrinter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "num_output_classes=10\n",
    "num_hidden_layers=1\n",
    "hidden_layers_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input variables denoting the features and label data\n",
    "feature = C.input_variable(input_dim, np.float32)\n",
    "label = C.input_variable(num_output_classes, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_input = element_times(constant(0.00390625), feature)\n",
    "\n",
    "z = Sequential([For(range(num_hidden_layers), lambda i: Dense(hidden_layers_dim, activation=relu)),\n",
    "                Dense(num_output_classes)])(scaled_input)\n",
    "\n",
    "ce = cross_entropy_with_softmax(z, label)\n",
    "pe = classification_error(z, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader_train = MinibatchSource(CTFDeserializer(train_path, StreamDefs(\n",
    "        features  = StreamDef(field='features', shape=input_dim, is_sparse=False),\n",
    "        labels    = StreamDef(field='labels',   shape=num_output_classes, is_sparse=False)\n",
    "    )), randomize=True, max_sweeps = INFINITELY_REPEAT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_map = {\n",
    "    feature  : reader_train.streams.features,\n",
    "    label  : reader_train.streams.labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training config\n",
    "minibatch_size = 64\n",
    "num_samples_per_sweep = 60000\n",
    "num_sweeps_to_train_with = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "progress_writers = [ProgressPrinter(\n",
    "    #freq=training_progress_output_freq,\n",
    "    tag='Training',\n",
    "    num_epochs=num_sweeps_to_train_with)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = learning_rate_schedule(1, UnitType.sample)\n",
    "\n",
    "trainer = Trainer(z, (ce, pe), adadelta(z.parameters, lr), progress_writers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per sample: 1.0\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.382969 * 60032, metric = 10.22% * 60032 42.944s (1397.9 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.202870 * 59968, metric = 5.72% * 59968 1.525s (39323.3 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.156793 * 60032, metric = 4.44% * 60032 3.592s (16712.7 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.128566 * 59968, metric = 3.65% * 59968 1.702s (35233.8 samples/s);\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.109716 * 60032, metric = 3.10% * 60032 1.704s (35230.0 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.095438 * 59968, metric = 2.69% * 59968 1.845s (32503.0 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.084263 * 60032, metric = 2.38% * 60032 1.852s (32414.7 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.075855 * 59968, metric = 2.11% * 59968 1.789s (33520.4 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.068660 * 60032, metric = 1.94% * 60032 1.663s (36098.6 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.062561 * 59968, metric = 1.72% * 59968 1.771s (33861.1 samples/s);\n"
     ]
    }
   ],
   "source": [
    "training_session(\n",
    "    trainer=trainer,\n",
    "    mb_source = reader_train,\n",
    "    mb_size = minibatch_size,\n",
    "    model_inputs_to_streams = input_map,\n",
    "    max_samples = num_samples_per_sweep * num_sweeps_to_train_with,\n",
    "    progress_frequency=num_samples_per_sweep\n",
    ").train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader_test =  MinibatchSource(CTFDeserializer(test_path, StreamDefs(\n",
    "        features  = StreamDef(field='features', shape=input_dim, is_sparse=False),\n",
    "        labels    = StreamDef(field='labels',   shape=num_output_classes, is_sparse=False)\n",
    "    )), randomize=False, max_sweeps = 1)\n",
    "\n",
    "input_map = {\n",
    "    feature  : reader_test.streams.features,\n",
    "    label  : reader_test.streams.labels\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0232\n"
     ]
    }
   ],
   "source": [
    "# Test data for trained model\n",
    "test_minibatch_size = 1024\n",
    "num_samples = 10000\n",
    "num_minibatches_to_test = num_samples / test_minibatch_size\n",
    "test_result = 0.0\n",
    "for i in range(0, int(num_minibatches_to_test)):\n",
    "    mb = reader_test.next_minibatch(test_minibatch_size, input_map=input_map)\n",
    "    eval_error = trainer.test_minibatch(mb)\n",
    "    test_result = test_result + eval_error\n",
    "\n",
    "# Average of evaluation errors of all test minibatchese\n",
    "eval = test_result / num_minibatches_to_test\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_431/lib/python3.6/site-packages/cntk/core.py:351: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input3\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.ndimage\n",
    "data = np.vectorize(lambda x: 255 - x)(np.ndarray.flatten(scipy.ndimage.imread(\"/home/nbuser/3.png\", flatten=True)))\n",
    "\n",
    "from cntk import load_model\n",
    "\n",
    "#loaded_model = load_model(\"my\", 'float')\n",
    "\n",
    "pred = np.squeeze(z.eval({feature:[data]}))\n",
    "\n",
    "\n",
    "top_class = np.argmax(pred)\n",
    "\n",
    "top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
